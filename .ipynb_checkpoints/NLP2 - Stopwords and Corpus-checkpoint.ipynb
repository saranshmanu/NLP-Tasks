{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aber',\n",
       " 'alle',\n",
       " 'allem',\n",
       " 'allen',\n",
       " 'aller',\n",
       " 'alles',\n",
       " 'als',\n",
       " 'also',\n",
       " 'am',\n",
       " 'an',\n",
       " 'ander',\n",
       " 'andere',\n",
       " 'anderem',\n",
       " 'anderen',\n",
       " 'anderer',\n",
       " 'anderes',\n",
       " 'anderm',\n",
       " 'andern',\n",
       " 'anderr',\n",
       " 'anders',\n",
       " 'auch',\n",
       " 'auf',\n",
       " 'aus',\n",
       " 'bei',\n",
       " 'bin',\n",
       " 'bis',\n",
       " 'bist',\n",
       " 'da',\n",
       " 'damit',\n",
       " 'dann',\n",
       " 'der',\n",
       " 'den',\n",
       " 'des',\n",
       " 'dem',\n",
       " 'die',\n",
       " 'das',\n",
       " 'daß',\n",
       " 'derselbe',\n",
       " 'derselben',\n",
       " 'denselben',\n",
       " 'desselben',\n",
       " 'demselben',\n",
       " 'dieselbe',\n",
       " 'dieselben',\n",
       " 'dasselbe',\n",
       " 'dazu',\n",
       " 'dein',\n",
       " 'deine',\n",
       " 'deinem',\n",
       " 'deinen',\n",
       " 'deiner',\n",
       " 'deines',\n",
       " 'denn',\n",
       " 'derer',\n",
       " 'dessen',\n",
       " 'dich',\n",
       " 'dir',\n",
       " 'du',\n",
       " 'dies',\n",
       " 'diese',\n",
       " 'diesem',\n",
       " 'diesen',\n",
       " 'dieser',\n",
       " 'dieses',\n",
       " 'doch',\n",
       " 'dort',\n",
       " 'durch',\n",
       " 'ein',\n",
       " 'eine',\n",
       " 'einem',\n",
       " 'einen',\n",
       " 'einer',\n",
       " 'eines',\n",
       " 'einig',\n",
       " 'einige',\n",
       " 'einigem',\n",
       " 'einigen',\n",
       " 'einiger',\n",
       " 'einiges',\n",
       " 'einmal',\n",
       " 'er',\n",
       " 'ihn',\n",
       " 'ihm',\n",
       " 'es',\n",
       " 'etwas',\n",
       " 'euer',\n",
       " 'eure',\n",
       " 'eurem',\n",
       " 'euren',\n",
       " 'eurer',\n",
       " 'eures',\n",
       " 'für',\n",
       " 'gegen',\n",
       " 'gewesen',\n",
       " 'hab',\n",
       " 'habe',\n",
       " 'haben',\n",
       " 'hat',\n",
       " 'hatte',\n",
       " 'hatten',\n",
       " 'hier',\n",
       " 'hin',\n",
       " 'hinter',\n",
       " 'ich',\n",
       " 'mich',\n",
       " 'mir',\n",
       " 'ihr',\n",
       " 'ihre',\n",
       " 'ihrem',\n",
       " 'ihren',\n",
       " 'ihrer',\n",
       " 'ihres',\n",
       " 'euch',\n",
       " 'im',\n",
       " 'in',\n",
       " 'indem',\n",
       " 'ins',\n",
       " 'ist',\n",
       " 'jede',\n",
       " 'jedem',\n",
       " 'jeden',\n",
       " 'jeder',\n",
       " 'jedes',\n",
       " 'jene',\n",
       " 'jenem',\n",
       " 'jenen',\n",
       " 'jener',\n",
       " 'jenes',\n",
       " 'jetzt',\n",
       " 'kann',\n",
       " 'kein',\n",
       " 'keine',\n",
       " 'keinem',\n",
       " 'keinen',\n",
       " 'keiner',\n",
       " 'keines',\n",
       " 'können',\n",
       " 'könnte',\n",
       " 'machen',\n",
       " 'man',\n",
       " 'manche',\n",
       " 'manchem',\n",
       " 'manchen',\n",
       " 'mancher',\n",
       " 'manches',\n",
       " 'mein',\n",
       " 'meine',\n",
       " 'meinem',\n",
       " 'meinen',\n",
       " 'meiner',\n",
       " 'meines',\n",
       " 'mit',\n",
       " 'muss',\n",
       " 'musste',\n",
       " 'nach',\n",
       " 'nicht',\n",
       " 'nichts',\n",
       " 'noch',\n",
       " 'nun',\n",
       " 'nur',\n",
       " 'ob',\n",
       " 'oder',\n",
       " 'ohne',\n",
       " 'sehr',\n",
       " 'sein',\n",
       " 'seine',\n",
       " 'seinem',\n",
       " 'seinen',\n",
       " 'seiner',\n",
       " 'seines',\n",
       " 'selbst',\n",
       " 'sich',\n",
       " 'sie',\n",
       " 'ihnen',\n",
       " 'sind',\n",
       " 'so',\n",
       " 'solche',\n",
       " 'solchem',\n",
       " 'solchen',\n",
       " 'solcher',\n",
       " 'solches',\n",
       " 'soll',\n",
       " 'sollte',\n",
       " 'sondern',\n",
       " 'sonst',\n",
       " 'über',\n",
       " 'um',\n",
       " 'und',\n",
       " 'uns',\n",
       " 'unsere',\n",
       " 'unserem',\n",
       " 'unseren',\n",
       " 'unser',\n",
       " 'unseres',\n",
       " 'unter',\n",
       " 'viel',\n",
       " 'vom',\n",
       " 'von',\n",
       " 'vor',\n",
       " 'während',\n",
       " 'war',\n",
       " 'waren',\n",
       " 'warst',\n",
       " 'was',\n",
       " 'weg',\n",
       " 'weil',\n",
       " 'weiter',\n",
       " 'welche',\n",
       " 'welchem',\n",
       " 'welchen',\n",
       " 'welcher',\n",
       " 'welches',\n",
       " 'wenn',\n",
       " 'werde',\n",
       " 'werden',\n",
       " 'wie',\n",
       " 'wieder',\n",
       " 'will',\n",
       " 'wir',\n",
       " 'wird',\n",
       " 'wirst',\n",
       " 'wo',\n",
       " 'wollen',\n",
       " 'wollte',\n",
       " 'würde',\n",
       " 'würden',\n",
       " 'zu',\n",
       " 'zum',\n",
       " 'zur',\n",
       " 'zwar',\n",
       " 'zwischen']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words('german')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['och',\n",
       " 'det',\n",
       " 'att',\n",
       " 'i',\n",
       " 'en',\n",
       " 'jag',\n",
       " 'hon',\n",
       " 'som',\n",
       " 'han',\n",
       " 'på',\n",
       " 'den',\n",
       " 'med',\n",
       " 'var',\n",
       " 'sig',\n",
       " 'för',\n",
       " 'så',\n",
       " 'till',\n",
       " 'är',\n",
       " 'men',\n",
       " 'ett',\n",
       " 'om',\n",
       " 'hade',\n",
       " 'de',\n",
       " 'av',\n",
       " 'icke',\n",
       " 'mig',\n",
       " 'du',\n",
       " 'henne',\n",
       " 'då',\n",
       " 'sin',\n",
       " 'nu',\n",
       " 'har',\n",
       " 'inte',\n",
       " 'hans',\n",
       " 'honom',\n",
       " 'skulle',\n",
       " 'hennes',\n",
       " 'där',\n",
       " 'min',\n",
       " 'man',\n",
       " 'ej',\n",
       " 'vid',\n",
       " 'kunde',\n",
       " 'något',\n",
       " 'från',\n",
       " 'ut',\n",
       " 'när',\n",
       " 'efter',\n",
       " 'upp',\n",
       " 'vi',\n",
       " 'dem',\n",
       " 'vara',\n",
       " 'vad',\n",
       " 'över',\n",
       " 'än',\n",
       " 'dig',\n",
       " 'kan',\n",
       " 'sina',\n",
       " 'här',\n",
       " 'ha',\n",
       " 'mot',\n",
       " 'alla',\n",
       " 'under',\n",
       " 'någon',\n",
       " 'eller',\n",
       " 'allt',\n",
       " 'mycket',\n",
       " 'sedan',\n",
       " 'ju',\n",
       " 'denna',\n",
       " 'själv',\n",
       " 'detta',\n",
       " 'åt',\n",
       " 'utan',\n",
       " 'varit',\n",
       " 'hur',\n",
       " 'ingen',\n",
       " 'mitt',\n",
       " 'ni',\n",
       " 'bli',\n",
       " 'blev',\n",
       " 'oss',\n",
       " 'din',\n",
       " 'dessa',\n",
       " 'några',\n",
       " 'deras',\n",
       " 'blir',\n",
       " 'mina',\n",
       " 'samma',\n",
       " 'vilken',\n",
       " 'er',\n",
       " 'sådan',\n",
       " 'vår',\n",
       " 'blivit',\n",
       " 'dess',\n",
       " 'inom',\n",
       " 'mellan',\n",
       " 'sådant',\n",
       " 'varför',\n",
       " 'varje',\n",
       " 'vilka',\n",
       " 'ditt',\n",
       " 'vem',\n",
       " 'vilket',\n",
       " 'sitta',\n",
       " 'sådana',\n",
       " 'vart',\n",
       " 'dina',\n",
       " 'vars',\n",
       " 'vårt',\n",
       " 'våra',\n",
       " 'ert',\n",
       " 'era',\n",
       " 'vilkas']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('swedish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['au',\n",
       " 'aux',\n",
       " 'avec',\n",
       " 'ce',\n",
       " 'ces',\n",
       " 'dans',\n",
       " 'de',\n",
       " 'des',\n",
       " 'du',\n",
       " 'elle',\n",
       " 'en',\n",
       " 'et',\n",
       " 'eux',\n",
       " 'il',\n",
       " 'je',\n",
       " 'la',\n",
       " 'le',\n",
       " 'leur',\n",
       " 'lui',\n",
       " 'ma',\n",
       " 'mais',\n",
       " 'me',\n",
       " 'même',\n",
       " 'mes',\n",
       " 'moi',\n",
       " 'mon',\n",
       " 'ne',\n",
       " 'nos',\n",
       " 'notre',\n",
       " 'nous',\n",
       " 'on',\n",
       " 'ou',\n",
       " 'par',\n",
       " 'pas',\n",
       " 'pour',\n",
       " 'qu',\n",
       " 'que',\n",
       " 'qui',\n",
       " 'sa',\n",
       " 'se',\n",
       " 'ses',\n",
       " 'son',\n",
       " 'sur',\n",
       " 'ta',\n",
       " 'te',\n",
       " 'tes',\n",
       " 'toi',\n",
       " 'ton',\n",
       " 'tu',\n",
       " 'un',\n",
       " 'une',\n",
       " 'vos',\n",
       " 'votre',\n",
       " 'vous',\n",
       " 'c',\n",
       " 'd',\n",
       " 'j',\n",
       " 'l',\n",
       " 'à',\n",
       " 'm',\n",
       " 'n',\n",
       " 's',\n",
       " 't',\n",
       " 'y',\n",
       " 'été',\n",
       " 'étée',\n",
       " 'étées',\n",
       " 'étés',\n",
       " 'étant',\n",
       " 'étante',\n",
       " 'étants',\n",
       " 'étantes',\n",
       " 'suis',\n",
       " 'es',\n",
       " 'est',\n",
       " 'sommes',\n",
       " 'êtes',\n",
       " 'sont',\n",
       " 'serai',\n",
       " 'seras',\n",
       " 'sera',\n",
       " 'serons',\n",
       " 'serez',\n",
       " 'seront',\n",
       " 'serais',\n",
       " 'serait',\n",
       " 'serions',\n",
       " 'seriez',\n",
       " 'seraient',\n",
       " 'étais',\n",
       " 'était',\n",
       " 'étions',\n",
       " 'étiez',\n",
       " 'étaient',\n",
       " 'fus',\n",
       " 'fut',\n",
       " 'fûmes',\n",
       " 'fûtes',\n",
       " 'furent',\n",
       " 'sois',\n",
       " 'soit',\n",
       " 'soyons',\n",
       " 'soyez',\n",
       " 'soient',\n",
       " 'fusse',\n",
       " 'fusses',\n",
       " 'fût',\n",
       " 'fussions',\n",
       " 'fussiez',\n",
       " 'fussent',\n",
       " 'ayant',\n",
       " 'ayante',\n",
       " 'ayantes',\n",
       " 'ayants',\n",
       " 'eu',\n",
       " 'eue',\n",
       " 'eues',\n",
       " 'eus',\n",
       " 'ai',\n",
       " 'as',\n",
       " 'avons',\n",
       " 'avez',\n",
       " 'ont',\n",
       " 'aurai',\n",
       " 'auras',\n",
       " 'aura',\n",
       " 'aurons',\n",
       " 'aurez',\n",
       " 'auront',\n",
       " 'aurais',\n",
       " 'aurait',\n",
       " 'aurions',\n",
       " 'auriez',\n",
       " 'auraient',\n",
       " 'avais',\n",
       " 'avait',\n",
       " 'avions',\n",
       " 'aviez',\n",
       " 'avaient',\n",
       " 'eut',\n",
       " 'eûmes',\n",
       " 'eûtes',\n",
       " 'eurent',\n",
       " 'aie',\n",
       " 'aies',\n",
       " 'ait',\n",
       " 'ayons',\n",
       " 'ayez',\n",
       " 'aient',\n",
       " 'eusse',\n",
       " 'eusses',\n",
       " 'eût',\n",
       " 'eussions',\n",
       " 'eussiez',\n",
       " 'eussent']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('french')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133737"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entries= nltk.corpus.cmudict.entries()\n",
    "len(entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', ['AH0']), ('a.', ['EY1']), ('a', ['EY1']), ...]\n"
     ]
    }
   ],
   "source": [
    "entries=nltk.corpus.cmudict.entries()\n",
    "print(entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('abdication', ['AE2', 'B', 'D', 'IH0', 'K', 'EY1', 'SH', 'AH0', 'N'])\n",
      "('abdnor', ['AE1', 'B', 'D', 'N', 'ER0'])\n",
      "('abdo', ['AE1', 'B', 'D', 'OW0'])\n",
      "('abdollah', ['AE2', 'B', 'D', 'AA1', 'L', 'AH0'])\n",
      "('abdomen', ['AE0', 'B', 'D', 'OW1', 'M', 'AH0', 'N'])\n",
      "('abdomen', ['AE1', 'B', 'D', 'AH0', 'M', 'AH0', 'N'])\n",
      "('abdominal', ['AE0', 'B', 'D', 'AA1', 'M', 'AH0', 'N', 'AH0', 'L'])\n",
      "('abdominal', ['AH0', 'B', 'D', 'AA1', 'M', 'AH0', 'N', 'AH0', 'L'])\n",
      "('abduct', ['AE0', 'B', 'D', 'AH1', 'K', 'T'])\n",
      "('abducted', ['AE0', 'B', 'D', 'AH1', 'K', 'T', 'IH0', 'D'])\n",
      "('abducted', ['AH0', 'B', 'D', 'AH1', 'K', 'T', 'IH0', 'D'])\n",
      "('abductee', ['AE0', 'B', 'D', 'AH2', 'K', 'T', 'IY1'])\n",
      "('abductees', ['AE0', 'B', 'D', 'AH2', 'K', 'T', 'IY1', 'Z'])\n",
      "('abducting', ['AE0', 'B', 'D', 'AH1', 'K', 'T', 'IH0', 'NG'])\n",
      "('abducting', ['AH0', 'B', 'D', 'AH1', 'K', 'T', 'IH0', 'NG'])\n",
      "('abduction', ['AE0', 'B', 'D', 'AH1', 'K', 'SH', 'AH0', 'N'])\n",
      "('abduction', ['AH0', 'B', 'D', 'AH1', 'K', 'SH', 'AH0', 'N'])\n",
      "('abductions', ['AE0', 'B', 'D', 'AH1', 'K', 'SH', 'AH0', 'N', 'Z'])\n",
      "('abductions', ['AH0', 'B', 'D', 'AH1', 'K', 'SH', 'AH0', 'N', 'Z'])\n",
      "('abductor', ['AE0', 'B', 'D', 'AH1', 'K', 'T', 'ER0'])\n",
      "('abductor', ['AH0', 'B', 'D', 'AH1', 'K', 'T', 'ER0'])\n",
      "('abductors', ['AE0', 'B', 'D', 'AH1', 'K', 'T', 'ER0', 'Z'])\n",
      "('abductors', ['AH0', 'B', 'D', 'AH1', 'K', 'T', 'ER0', 'Z'])\n",
      "('abducts', ['AE0', 'B', 'D', 'AH1', 'K', 'T', 'S'])\n",
      "('abdul', ['AE0', 'B', 'D', 'UW1', 'L'])\n",
      "('abdulaziz', ['AE0', 'B', 'D', 'UW2', 'L', 'AH0', 'Z', 'IY1', 'Z'])\n",
      "('abdulla', ['AA0', 'B', 'D', 'UW1', 'L', 'AH0'])\n",
      "('abdullah', ['AE2', 'B', 'D', 'AH1', 'L', 'AH0'])\n",
      "('abe', ['EY1', 'B'])\n",
      "('abed', ['AH0', 'B', 'EH1', 'D'])\n"
     ]
    }
   ],
   "source": [
    "for i in entries[100:130]:\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('car.n.01')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "wn.synsets('motorcar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['car', 'auto', 'automobile', 'machine', 'motorcar']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn .synset('car.n.01').lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('asset.n.01'),\n",
       " Synset('summation.n.04'),\n",
       " Synset('plus.a.01'),\n",
       " Synset('plus.s.02')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('plus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['plus']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('plus.a.01').lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hello', 'NN'), ('my', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), ('dennis', 'JJ'), ('i', 'RB'), ('am', 'VBP'), ('studying', 'VBG'), ('in', 'IN'), ('VIT', 'NNP'), ('good', 'JJ'), ('day', 'NN'), ('to', 'TO'), ('you', 'PRP')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "texts=[\"hello my name is dennis\"\" i am studying in VIT \"\"good day to you\"]\n",
    "for text in texts:\n",
    "    sentences=nltk.sent_tokenize(text)\n",
    "    for sentence in sentences:\n",
    "        words= nltk.word_tokenize(sentence)\n",
    "        tagged_words=nltk.pos_tag(words)\n",
    "        print(tagged_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'party', 'was', 'lit', 'XD', '#livingthelife', '#breakrulesallday']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "text='The party was lit XD #livingthelife #breakrulesallday'\n",
    "twtkn=TweetTokenizer()\n",
    "twtkn.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import inaugural\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on PlaintextCorpusReader in module nltk.corpus.reader.plaintext object:\n",
      "\n",
      "class PlaintextCorpusReader(nltk.corpus.reader.api.CorpusReader)\n",
      " |  PlaintextCorpusReader(root, fileids, word_tokenizer=WordPunctTokenizer(pattern='\\\\w+|[^\\\\w\\\\s]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>), sent_tokenizer=<nltk.tokenize.punkt.PunktSentenceTokenizer object at 0x000001E32B3CEE10>, para_block_reader=<function read_blankline_block at 0x000001E32B3E48C8>, encoding='utf8')\n",
      " |  \n",
      " |  Reader for corpora that consist of plaintext documents.  Paragraphs\n",
      " |  are assumed to be split using blank lines.  Sentences and words can\n",
      " |  be tokenized using the default tokenizers, or by custom tokenizers\n",
      " |  specificed as parameters to the constructor.\n",
      " |  \n",
      " |  This corpus reader can be customized (e.g., to skip preface\n",
      " |  sections of specific document formats) by creating a subclass and\n",
      " |  overriding the ``CorpusView`` class variable.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      PlaintextCorpusReader\n",
      " |      nltk.corpus.reader.api.CorpusReader\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, root, fileids, word_tokenizer=WordPunctTokenizer(pattern='\\\\w+|[^\\\\w\\\\s]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>), sent_tokenizer=<nltk.tokenize.punkt.PunktSentenceTokenizer object at 0x000001E32B3CEE10>, para_block_reader=<function read_blankline_block at 0x000001E32B3E48C8>, encoding='utf8')\n",
      " |      Construct a new plaintext corpus reader for a set of documents\n",
      " |      located at the given root directory.  Example usage:\n",
      " |      \n",
      " |          >>> root = '/usr/local/share/nltk_data/corpora/webtext/'\n",
      " |          >>> reader = PlaintextCorpusReader(root, '.*\\.txt') # doctest: +SKIP\n",
      " |      \n",
      " |      :param root: The root directory for this corpus.\n",
      " |      :param fileids: A list or regexp specifying the fileids in this corpus.\n",
      " |      :param word_tokenizer: Tokenizer for breaking sentences or\n",
      " |          paragraphs into words.\n",
      " |      :param sent_tokenizer: Tokenizer for breaking paragraphs\n",
      " |          into words.\n",
      " |      :param para_block_reader: The block reader used to divide the\n",
      " |          corpus into paragraph blocks.\n",
      " |  \n",
      " |  paras(self, fileids=None)\n",
      " |      :return: the given file(s) as a list of\n",
      " |          paragraphs, each encoded as a list of sentences, which are\n",
      " |          in turn encoded as lists of word strings.\n",
      " |      :rtype: list(list(list(str)))\n",
      " |  \n",
      " |  raw(self, fileids=None)\n",
      " |      :return: the given file(s) as a single string.\n",
      " |      :rtype: str\n",
      " |  \n",
      " |  sents(self, fileids=None)\n",
      " |      :return: the given file(s) as a list of\n",
      " |          sentences or utterances, each encoded as a list of word\n",
      " |          strings.\n",
      " |      :rtype: list(list(str))\n",
      " |  \n",
      " |  words(self, fileids=None)\n",
      " |      :return: the given file(s) as a list of words\n",
      " |          and punctuation symbols.\n",
      " |      :rtype: list(str)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  CorpusView = <class 'nltk.corpus.reader.util.StreamBackedCorpusView'>\n",
      " |      A 'view' of a corpus file, which acts like a sequence of tokens:\n",
      " |      it can be accessed by index, iterated over, etc.  However, the\n",
      " |      tokens are only constructed as-needed -- the entire corpus is\n",
      " |      never stored in memory at once.\n",
      " |      \n",
      " |      The constructor to ``StreamBackedCorpusView`` takes two arguments:\n",
      " |      a corpus fileid (specified as a string or as a ``PathPointer``);\n",
      " |      and a block reader.  A \"block reader\" is a function that reads\n",
      " |      zero or more tokens from a stream, and returns them as a list.  A\n",
      " |      very simple example of a block reader is:\n",
      " |      \n",
      " |          >>> def simple_block_reader(stream):\n",
      " |          ...     return stream.readline().split()\n",
      " |      \n",
      " |      This simple block reader reads a single line at a time, and\n",
      " |      returns a single token (consisting of a string) for each\n",
      " |      whitespace-separated substring on the line.\n",
      " |      \n",
      " |      When deciding how to define the block reader for a given\n",
      " |      corpus, careful consideration should be given to the size of\n",
      " |      blocks handled by the block reader.  Smaller block sizes will\n",
      " |      increase the memory requirements of the corpus view's internal\n",
      " |      data structures (by 2 integers per block).  On the other hand,\n",
      " |      larger block sizes may decrease performance for random access to\n",
      " |      the corpus.  (But note that larger block sizes will *not*\n",
      " |      decrease performance for iteration.)\n",
      " |      \n",
      " |      Internally, ``CorpusView`` maintains a partial mapping from token\n",
      " |      index to file position, with one entry per block.  When a token\n",
      " |      with a given index *i* is requested, the ``CorpusView`` constructs\n",
      " |      it as follows:\n",
      " |      \n",
      " |        1. First, it searches the toknum/filepos mapping for the token\n",
      " |           index closest to (but less than or equal to) *i*.\n",
      " |      \n",
      " |        2. Then, starting at the file position corresponding to that\n",
      " |           index, it reads one block at a time using the block reader\n",
      " |           until it reaches the requested token.\n",
      " |      \n",
      " |      The toknum/filepos mapping is created lazily: it is initially\n",
      " |      empty, but every time a new block is read, the block's\n",
      " |      initial token is added to the mapping.  (Thus, the toknum/filepos\n",
      " |      map has one entry per block.)\n",
      " |      \n",
      " |      In order to increase efficiency for random access patterns that\n",
      " |      have high degrees of locality, the corpus view may cache one or\n",
      " |      more blocks.\n",
      " |      \n",
      " |      :note: Each ``CorpusView`` object internally maintains an open file\n",
      " |          object for its underlying corpus file.  This file should be\n",
      " |          automatically closed when the ``CorpusView`` is garbage collected,\n",
      " |          but if you wish to close it manually, use the ``close()``\n",
      " |          method.  If you access a ``CorpusView``'s items after it has been\n",
      " |          closed, the file object will be automatically re-opened.\n",
      " |      \n",
      " |      :warning: If the contents of the file are modified during the\n",
      " |          lifetime of the ``CorpusView``, then the ``CorpusView``'s behavior\n",
      " |          is undefined.\n",
      " |      \n",
      " |      :warning: If a unicode encoding is specified when constructing a\n",
      " |          ``CorpusView``, then the block reader may only call\n",
      " |          ``stream.seek()`` with offsets that have been returned by\n",
      " |          ``stream.tell()``; in particular, calling ``stream.seek()`` with\n",
      " |          relative offsets, or with offsets based on string lengths, may\n",
      " |          lead to incorrect behavior.\n",
      " |      \n",
      " |      :ivar _block_reader: The function used to read\n",
      " |          a single block from the underlying file stream.\n",
      " |      :ivar _toknum: A list containing the token index of each block\n",
      " |          that has been processed.  In particular, ``_toknum[i]`` is the\n",
      " |          token index of the first token in block ``i``.  Together\n",
      " |          with ``_filepos``, this forms a partial mapping between token\n",
      " |          indices and file positions.\n",
      " |      :ivar _filepos: A list containing the file position of each block\n",
      " |          that has been processed.  In particular, ``_toknum[i]`` is the\n",
      " |          file position of the first character in block ``i``.  Together\n",
      " |          with ``_toknum``, this forms a partial mapping between token\n",
      " |          indices and file positions.\n",
      " |      :ivar _stream: The stream used to access the underlying corpus file.\n",
      " |      :ivar _len: The total number of tokens in the corpus, if known;\n",
      " |          or None, if the number of tokens is not yet known.\n",
      " |      :ivar _eofpos: The character position of the last character in the\n",
      " |          file.  This is calculated when the corpus view is initialized,\n",
      " |          and is used to decide when the end of file has been reached.\n",
      " |      :ivar _cache: A cache of the most recently read block.  It\n",
      " |         is encoded as a tuple (start_toknum, end_toknum, tokens), where\n",
      " |         start_toknum is the token index of the first token in the block;\n",
      " |         end_toknum is the token index of the first token not in the\n",
      " |         block; and tokens is a list of the tokens in the block.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __unicode__ = __str__(self, /)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  abspath(self, fileid)\n",
      " |      Return the absolute path for the given file.\n",
      " |      \n",
      " |      :type fileid: str\n",
      " |      :param fileid: The file identifier for the file whose path\n",
      " |          should be returned.\n",
      " |      :rtype: PathPointer\n",
      " |  \n",
      " |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      " |      Return a list of the absolute paths for all fileids in this corpus;\n",
      " |      or for the given list of fileids, if specified.\n",
      " |      \n",
      " |      :type fileids: None or str or list\n",
      " |      :param fileids: Specifies the set of fileids for which paths should\n",
      " |          be returned.  Can be None, for all fileids; a list of\n",
      " |          file identifiers, for a specified set of fileids; or a single\n",
      " |          file identifier, for a single file.  Note that the return\n",
      " |          value is always a list of paths, even if ``fileids`` is a\n",
      " |          single file identifier.\n",
      " |      \n",
      " |      :param include_encoding: If true, then return a list of\n",
      " |          ``(path_pointer, encoding)`` tuples.\n",
      " |      \n",
      " |      :rtype: list(PathPointer)\n",
      " |  \n",
      " |  citation(self)\n",
      " |      Return the contents of the corpus citation.bib file, if it exists.\n",
      " |  \n",
      " |  encoding(self, file)\n",
      " |      Return the unicode encoding for the given corpus file, if known.\n",
      " |      If the encoding is unknown, or if the given file should be\n",
      " |      processed using byte strings (str), then return None.\n",
      " |  \n",
      " |  ensure_loaded(self)\n",
      " |      Load this corpus (if it has not already been loaded).  This is\n",
      " |      used by LazyCorpusLoader as a simple method that can be used to\n",
      " |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      " |      do help(some_corpus).\n",
      " |  \n",
      " |  fileids(self)\n",
      " |      Return a list of file identifiers for the fileids that make up\n",
      " |      this corpus.\n",
      " |  \n",
      " |  license(self)\n",
      " |      Return the contents of the corpus LICENSE file, if it exists.\n",
      " |  \n",
      " |  open(self, file)\n",
      " |      Return an open stream that can be used to read the given file.\n",
      " |      If the file's encoding is not None, then the stream will\n",
      " |      automatically decode the file's contents into unicode.\n",
      " |      \n",
      " |      :param file: The file identifier of the file to read.\n",
      " |  \n",
      " |  readme(self)\n",
      " |      Return the contents of the corpus README file, if it exists.\n",
      " |  \n",
      " |  unicode_repr = __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  root\n",
      " |      The directory where this corpus is stored.\n",
      " |      \n",
      " |      :type: PathPointer\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help (inaugural)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['They', 'neither', 'liked', 'nor', 'disliked', 'the', ...]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.words(categories='romance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thirty-three', 'Scotty', 'did', 'not', 'go', 'back', ...]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.words(categories='fiction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1789-Washington.txt',\n",
       " '1793-Washington.txt',\n",
       " '1797-Adams.txt',\n",
       " '1801-Jefferson.txt',\n",
       " '1805-Jefferson.txt',\n",
       " '1809-Madison.txt',\n",
       " '1813-Madison.txt',\n",
       " '1817-Monroe.txt',\n",
       " '1821-Monroe.txt',\n",
       " '1825-Adams.txt',\n",
       " '1829-Jackson.txt',\n",
       " '1833-Jackson.txt',\n",
       " '1837-VanBuren.txt',\n",
       " '1841-Harrison.txt',\n",
       " '1845-Polk.txt',\n",
       " '1849-Taylor.txt',\n",
       " '1853-Pierce.txt',\n",
       " '1857-Buchanan.txt',\n",
       " '1861-Lincoln.txt',\n",
       " '1865-Lincoln.txt',\n",
       " '1869-Grant.txt',\n",
       " '1873-Grant.txt',\n",
       " '1877-Hayes.txt',\n",
       " '1881-Garfield.txt',\n",
       " '1885-Cleveland.txt',\n",
       " '1889-Harrison.txt',\n",
       " '1893-Cleveland.txt',\n",
       " '1897-McKinley.txt',\n",
       " '1901-McKinley.txt',\n",
       " '1905-Roosevelt.txt',\n",
       " '1909-Taft.txt',\n",
       " '1913-Wilson.txt',\n",
       " '1917-Wilson.txt',\n",
       " '1921-Harding.txt',\n",
       " '1925-Coolidge.txt',\n",
       " '1929-Hoover.txt',\n",
       " '1933-Roosevelt.txt',\n",
       " '1937-Roosevelt.txt',\n",
       " '1941-Roosevelt.txt',\n",
       " '1945-Roosevelt.txt',\n",
       " '1949-Truman.txt',\n",
       " '1953-Eisenhower.txt',\n",
       " '1957-Eisenhower.txt',\n",
       " '1961-Kennedy.txt',\n",
       " '1965-Johnson.txt',\n",
       " '1969-Nixon.txt',\n",
       " '1973-Nixon.txt',\n",
       " '1977-Carter.txt',\n",
       " '1981-Reagan.txt',\n",
       " '1985-Reagan.txt',\n",
       " '1989-Bush.txt',\n",
       " '1993-Clinton.txt',\n",
       " '1997-Clinton.txt',\n",
       " '2001-Bush.txt',\n",
       " '2005-Bush.txt',\n",
       " '2009-Obama.txt']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inaugural.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('hello.n.01')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "wn.synsets('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The five most frequent words are: \n",
      "the: 126\n",
      "and: 105\n",
      "of: 82\n",
      "to: 66\n",
      "our: 58\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import inaugural \n",
    "\n",
    "#import Obama's speech\n",
    "Obama = inaugural.words(fileids = '2009-Obama.txt')\n",
    "\n",
    "#declaring a dictionary for word frequency\n",
    "word_freq = {} \n",
    "for tok in Obama:\n",
    "    if tok in word_freq:\n",
    "        word_freq[tok] += 1\n",
    "    else:\n",
    "        word_freq[tok] = 1\n",
    "\n",
    "#finding top five most frequent words\n",
    "max_dict = {}\n",
    "while len(max_dict) < 5:\n",
    "    max_val = 0\n",
    "    for key in word_freq:\n",
    "        if max_val < word_freq[key] and re.match(r'[A-Za-z]+',key) and key not in max_dict:\n",
    "            max_key = key\n",
    "            max_val = word_freq[key]\n",
    "    max_dict[max_key] = max_val\n",
    "\n",
    "#displaying frequency distribution\n",
    "print(\"The five most frequent words are: \")\n",
    "for key in max_dict:\n",
    "    print(key+\":\",max_dict[key])\n",
    "    \n",
    "#plotting\n",
    "fd = nltk.FreqDist(Obama)\n",
    "fd.plot(30,cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-76a01d9c502b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Babylon/NNP)\n",
      "  was/VBD\n",
      "  a/DT\n",
      "  significant/JJ\n",
      "  city/NN\n",
      "  in/IN\n",
      "  ancient/JJ\n",
      "  Mesopotamia/NNP\n",
      "  ,/,\n",
      "  in/IN\n",
      "  the/DT\n",
      "  fertile/NN\n",
      "  plain/NN\n",
      "  between/IN\n",
      "  the/DT\n",
      "  (GPE Tigris/NNP)\n",
      "  and/CC\n",
      "  (GPE Euphrates/NNP)\n",
      "  rivers/NNS\n",
      "  ./.)\n",
      "(S\n",
      "  The/DT\n",
      "  city/NN\n",
      "  was/VBD\n",
      "  built/VBN\n",
      "  upon/IN\n",
      "  the/DT\n",
      "  (ORGANIZATION Euphrates/NNP)\n",
      "  ,/,\n",
      "  and/CC\n",
      "  divided/VBD\n",
      "  in/IN\n",
      "  equal/JJ\n",
      "  parts/NNS\n",
      "  along/IN\n",
      "  its/PRP$\n",
      "  left/NN\n",
      "  and/CC\n",
      "  right/JJ\n",
      "  banks/NNS\n",
      "  ,/,\n",
      "  with/IN\n",
      "  steep/JJ\n",
      "  embankments/NNS\n",
      "  to/TO\n",
      "  contain/VB\n",
      "  the/DT\n",
      "  river/NN\n",
      "  's/POS\n",
      "  seasonal/JJ\n",
      "  floods/NNS\n",
      "  ./.)\n",
      "(S\n",
      "  (PERSON Hammurabi/NNP)\n",
      "  was/VBD\n",
      "  the/DT\n",
      "  sixth/JJ\n",
      "  (ORGANIZATION Amorite/NNP)\n",
      "  king/NN\n",
      "  of/IN\n",
      "  (PERSON Babylon/NNP)\n",
      "  from/IN\n",
      "  1792/CD\n",
      "  BC/NNP\n",
      "  to/TO\n",
      "  1750/CD\n",
      "  BC/NNP\n",
      "  middle/JJ\n",
      "  chronology/NN\n",
      "  ./.)\n",
      "(S\n",
      "  He/PRP\n",
      "  became/VBD\n",
      "  the/DT\n",
      "  first/JJ\n",
      "  king/NN\n",
      "  of/IN\n",
      "  the/DT\n",
      "  (ORGANIZATION Babylonian/NNP)\n",
      "  Empire/NNP\n",
      "  following/VBG\n",
      "  the/DT\n",
      "  abdication/NN\n",
      "  of/IN\n",
      "  his/PRP$\n",
      "  father/NN\n",
      "  ,/,\n",
      "  Sin-Muballit/NNP\n",
      "  ,/,\n",
      "  who/WP\n",
      "  had/VBD\n",
      "  become/VBN\n",
      "  very/RB\n",
      "  ill/JJ\n",
      "  and/CC\n",
      "  died/VBD\n",
      "  ,/,\n",
      "  extending/VBG\n",
      "  (PERSON Babylon/NNP)\n",
      "  's/POS\n",
      "  control/NN\n",
      "  over/IN\n",
      "  (GPE Mesopotamia/NNP)\n",
      "  by/IN\n",
      "  winning/VBG\n",
      "  a/DT\n",
      "  series/NN\n",
      "  of/IN\n",
      "  wars/NNS\n",
      "  against/IN\n",
      "  neighboring/VBG\n",
      "  kingdoms/NNS\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    \"\"\"\n",
    "    Babylon was a significant city in ancient Mesopotamia,\n",
    "    in the fertile plain between the Tigris and Euphrates rivers.\n",
    "    The city was built upon the Euphrates, and divided in equal parts along its left and right banks,\n",
    "    with steep embankments to contain the river's seasonal floods.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Hammurabi was the sixth Amorite king of Babylon\n",
    "    from 1792 BC to 1750 BC middle chronology.\n",
    "    He became the first king of the Babylonian Empire following the abdication of his father,\n",
    "    Sin-Muballit, who had become very ill and died, extending Babylon's control over Mesopotamia\n",
    "    by winning a series of wars against neighboring kingdoms.\n",
    "    \"\"\"\n",
    "] \n",
    " \n",
    "for text in texts:\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    for sentence in sentences:\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        tagged_words = nltk.pos_tag(words)\n",
    "        ne_tagged_words = nltk.ne_chunk(tagged_words)\n",
    "        print (ne_tagged_words)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Dennis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
